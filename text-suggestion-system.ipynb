{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9274964,"sourceType":"datasetVersion","datasetId":5613380},{"sourceId":10486232,"sourceType":"datasetVersion","datasetId":6492627}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"notebookId":"53997d2d-afb8-4477-8874-b6d46299f06c","notebookPath":"seminar.ipynb"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter, defaultdict\nimport re\nfrom tqdm.auto import tqdm\nfrom typing import List, Union, Tuple\n\n\nclass PrefixTreeNode:\n    \"\"\"\n    Узел для префиксного дерева.\n    Хранит детей узла и флаг конца слова.\n    \"\"\"\n    def __init__(self):\n        self.children = {}  # Дочерние узлы\n        self.is_end_of_word = False  # Является ли конец слова\n\n\nclass PrefixTree:\n    \"\"\"\n    Префиксное дерево (трис) для хранения слов и поиска по префиксам.\n    \"\"\"\n    def __init__(self, vocabulary: List[str]):\n        self.root = PrefixTreeNode()\n        for word in vocabulary:\n            self.insert(word)\n\n    def insert(self, word: str):\n        \"\"\"\n        Добавляет слово в префиксное дерево.\n\n        :param word: Слово для вставки\n        \"\"\"\n        node = self.root\n        for char in word:\n            if char not in node.children:\n                node.children[char] = PrefixTreeNode()\n            node = node.children[char]\n        node.is_end_of_word = True\n\n    def _collect_words(self, node: PrefixTreeNode, prefix: str) -> List[str]:\n        \"\"\"\n        Рекурсивно собирает слова, которые начинаются с заданного префикса.\n\n        :param node: Текущий узел дерева\n        :param prefix: Текущий префикс\n        :return: Список слов\n        \"\"\"\n        words = []\n        if node.is_end_of_word:\n            words.append(prefix)\n        for char, child_node in node.children.items():\n            words.extend(self._collect_words(child_node, prefix + char))\n        return words\n\n    def search_prefix(self, prefix: str) -> List[str]:\n        \"\"\"\n        Находит все слова, начинающиеся с данного префикса.\n\n        :param prefix: Префикс для поиска\n        :return: Список слов, начинающихся с префикса\n        \"\"\"\n        node = self.root\n        for char in prefix:\n            if char in node.children:\n                node = node.children[char]\n            else:\n                return []\n        return self._collect_words(node, prefix)\n\n\nclass WordCompletor:\n    \"\"\"\n    Класс для дополнения слов на основе частоты их использования и префиксного дерева.\n    \"\"\"\n    def __init__(self, corpus: List[List[str]] = None, word_freqs: Counter = None, min_wf: int = 5):\n        \"\"\"\n        Инициализирует WordCompletor.\n\n        :param corpus: Корпус текстов для подсчета частоты слов\n        :param word_freqs: Готовые частоты слов (если есть)\n        :param min_wf: Минимальная частота слова для включения\n        \"\"\"\n        if word_freqs is not None:\n            self.word_freqs = word_freqs\n        else:\n            self.word_freqs = Counter()\n            for text in tqdm(corpus, desc=\"Calculating word frequencies\"):\n                self.word_freqs.update(Counter(text))\n\n            # Удаление слов с частотой меньше min_wf\n            self.word_freqs = {word: count for word, count in self.word_freqs.items() if count >= min_wf}\n\n            # Нормализация частот\n            total = sum(self.word_freqs.values())\n            for k in self.word_freqs:\n                self.word_freqs[k] /= total\n\n        self.prefix_tree = PrefixTree(self.word_freqs.keys())\n\n    def get_words_and_probs(self, prefix: str) -> Tuple[List[str], List[float]]:\n        \"\"\"\n        Возвращает слова и их вероятности для заданного префикса.\n\n        :param prefix: Префикс для поиска\n        :return: Список слов и их вероятности\n        \"\"\"\n        words = self.prefix_tree.search_prefix(prefix)\n        if not words:\n            return [], []\n        probs = [self.word_freqs[word] for word in words]\n        return words, probs\n\n\nclass NGramLanguageModel:\n    \"\"\"\n    N-граммная языковая модель для предсказания следующего слова.\n    \"\"\"\n    def __init__(self, corpus: List[List[str]] = None, n_gram_freq: defaultdict = None, n: int = 3, min_wf: int = 4):\n        \"\"\"\n        Инициализирует NGramLanguageModel.\n\n        :param corpus: Корпус текстов для подсчета n-грамм\n        :param n_gram_freq: Частоты n-грамм (если есть)\n        :param n: Длина n-грамм\n        :param min_wf: Минимальная частота для включения\n        \"\"\"\n        self.n = n\n        self.min_wf = min_wf\n        self.n_gram_freq = n_gram_freq or defaultdict(Counter)\n        if corpus:\n            self.count_ngrams(corpus)\n\n    def count_ngrams(self, corpus: List[List[str]]):\n        \"\"\"\n        Считает частоты n-грамм в корпусе.\n\n        :param corpus: Корпус текстов\n        \"\"\"\n        for line in tqdm(corpus, desc=\"Counting n-grams\"):\n            for i in range(self.n, len(line)):\n                prefix = tuple(line[i-self.n:i])\n                token = line[i]\n                self.n_gram_freq[prefix][token] += 1\n\n        # Фильтрация редких n-грамм и расчет вероятностей\n        for prefix in list(self.n_gram_freq.keys()):\n            token_counter = self.n_gram_freq[prefix]\n            filtered_tokens = {word: count for word, count in token_counter.items() if count >= self.min_wf}\n            if filtered_tokens:\n                total_count = sum(filtered_tokens.values())\n                self.n_gram_freq[prefix] = {word: count / total_count for word, count in filtered_tokens.items()}\n            else:\n                del self.n_gram_freq[prefix]\n\n    def get_last_n_tokens(self, prefix: List[str]) -> Tuple[str, ...]:\n        \"\"\"\n        Возвращает последние n токенов из списка.\n\n        :param prefix: Список токенов\n        :return: Кортеж последних n токенов\n        \"\"\"\n        return tuple(prefix[-self.n:])\n\n    def get_next_words_and_probs(self, prefix: List[str]) -> Tuple[List[str], List[float]]:\n        \"\"\"\n        Возвращает слова и их вероятности для продолжения префикса.\n\n        :param prefix: Префикс для поиска\n        :return: Список слов и их вероятности\n        \"\"\"\n        prefix = self.get_last_n_tokens(prefix)\n        if prefix in self.n_gram_freq:\n            possible_tokens = self.n_gram_freq[prefix]\n            next_words = list(possible_tokens.keys())\n            probs = list(possible_tokens.values())\n            return next_words, probs\n        return [], []\n\n\nclass TextSuggestion:\n    \"\"\"\n    Класс для предложения текстовых дополнений на основе модели.\n    \"\"\"\n    def __init__(self, word_completor: WordCompletor, n_gram_model: NGramLanguageModel):\n        self.word_completor = word_completor\n        self.n_gram_model = n_gram_model\n\n    @staticmethod\n    def tokenize(text: str) -> List[str]:\n        \"\"\"\n        Токенизирует текст.\n\n        :param text: Исходный текст\n        :return: Список токенов\n        \"\"\"\n        reg = re.compile(r'\\w+')\n        return reg.findall(text.lower())\n\n    def suggest_text(self, text: Union[str, List[str]], n_words: int = 3) -> List[str]:\n        \"\"\"\n        Предлагает продолжение текста.\n\n        :param text: Исходный текст или список токенов\n        :param n_words: Количество слов для предложения\n        :return: Список предложенных слов\n        \"\"\"\n        if isinstance(text, str):\n            text = self.tokenize(text)\n\n        suggestions = []\n        last_text_token = text[-1]\n\n        words, probs = self.word_completor.get_words_and_probs(last_text_token)\n        if words:\n            max_index = np.argmax(probs)\n            key_word = words[max_index]\n            suggestions.append(key_word)\n            text[-1] = key_word\n\n            for _ in range(n_words):\n                words, probs = self.n_gram_model.get_next_words_and_probs(text)\n                if words:\n                    max_index = np.argmax(probs)\n                    key_word = words[max_index]\n                    text.append(key_word)\n                    suggestions.append(key_word)\n                else:\n                    break\n\n        return suggestions\n\n\ndef main():\n    \"\"\"\n    Основная функция для демонстрации работы классов.\n    \"\"\"\n    # Загрузка данных\n    with open('/kaggle/input/emails/emails.txt', 'r') as f:\n        emails = [text.split() for text in f.read().split('\\n')]\n\n    # Инициализация классов\n    word_completor = WordCompletor(emails, min_wf=5)\n    n_gram_model = NGramLanguageModel(corpus=emails, n=3, min_wf=4)\n    text_suggestion = TextSuggestion(word_completor, n_gram_model)\n\n    # Пример использования\n    input_text = \"can we speed up t\"\n    suggestions = text_suggestion.suggest_text(input_text)\n    print(\"Предложения:\", suggestions)\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T11:32:34.222505Z","iopub.execute_input":"2025-01-16T11:32:34.223050Z","iopub.status.idle":"2025-01-16T11:35:12.552938Z","shell.execute_reply.started":"2025-01-16T11:32:34.222929Z","shell.execute_reply":"2025-01-16T11:35:12.551650Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Calculating word frequencies:   0%|          | 0/308361 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543537690d3a47dea54466d11317e04b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Counting n-grams:   0%|          | 0/308361 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d3dd37242b4d29a60bb6082c8e1095"}},"metadata":{}},{"name":"stdout","text":"Предложения: ['the', 'process', 'of', 'getting']\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}